{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40369ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-05-02 01:58:48--  https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 34.64.4.112, 34.64.4.16, 34.64.4.48, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|34.64.4.112|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 662903077 (632M) [application/zip]\n",
      "Saving to: ‘multi_cased_L-12_H-768_A-12.zip’\n",
      "\n",
      "        multi_cased  52%[=========>          ] 332.01M  15.1MB/s    eta 21s    ^C\n"
     ]
    },
    {
     "ename": "BadZipFile",
     "evalue": "File is not a zip file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-530c32cedfd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mbert_zip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multi_cased_L-12_H-768_A-12.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mbert_zip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel)\u001b[0m\n\u001b[1;32m   1256\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1258\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_RealGetContents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1259\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1260\u001b[0m                 \u001b[0;31m# set the modified flag so central directory gets written\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36m_RealGetContents\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1323\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File is not a zip file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mendrec\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1325\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File is not a zip file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1326\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendrec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBadZipFile\u001b[0m: File is not a zip file"
     ]
    }
   ],
   "source": [
    "#use wget to get bert model\n",
    "\n",
    "import os\n",
    "!wget https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip\n",
    "\n",
    "if \"bert\" not in os.listdir():\n",
    "  os.makedirs(\"bert\")\n",
    "else:\n",
    "  pass\n",
    "\n",
    "import zipfile\n",
    "import shutil\n",
    "         \n",
    "bert_zip = zipfile.ZipFile('multi_cased_L-12_H-768_A-12.zip')\n",
    "bert_zip.extractall('bert')\n",
    " \n",
    "bert_zip.close()\n",
    "\n",
    "def copytree(src, dst, symlinks=False, ignore=None):\n",
    "    for item in os.listdir(src):\n",
    "        s = os.path.join(src, item)\n",
    "        d = os.path.join(dst, item)\n",
    "        if os.path.isdir(s):\n",
    "            shutil.copytree(s, d, symlinks, ignore)\n",
    "        else:\n",
    "            shutil.copy2(s, d)\n",
    "\n",
    "copytree(\"bert/multi_cased_L-12_H-768_A-12\", \"bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0c32e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using os to import bert model\n",
    "import os\n",
    "\n",
    "os.listdir('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659be5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import requirements\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import re\n",
    "import pickle\n",
    "\n",
    "import keras as keras\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "from keras import Input, Model\n",
    "from keras import optimizers\n",
    "\n",
    "import codecs\n",
    "from tqdm import tqdm\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b426c5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install bert model\n",
    "%pip install keras-bert\n",
    "%pip install keras-radam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2a5764",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import model, activationfunction, tokenizer\n",
    "from keras_bert import load_trained_model_from_checkpoint, load_vocabulary\n",
    "from keras_bert import Tokenizer\n",
    "from keras_bert import AdamWarmup, calc_train_steps\n",
    "\n",
    "from keras_radam import RAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5363b8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_table(\"경로\"+\"파일명.txt\")\n",
    "test = pd.read_table(\"경로\"+\"파일명.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadf4b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting model dictionary\n",
    "'''\n",
    "SEQ_LEN = 문장 최대 길이 \n",
    "SEQ_LEN 이 최대 길이보다 작다면 남은 부분은 0으로 채워지고, 길다면 자름\n",
    "BATCH_SIZE = 16으로 정한 후 조정\n",
    "EPOCHS = 2 -> 대부분 에폭2단에서 좋은 결과물을 추출함\n",
    "Learning Rate는 1e-5로 초기설정 후 튜닝\n",
    "pretrained_bert 는 사전학습 모델이 있는 폴더를 의미\n",
    "긍정 및 부정인지 알려주는 칼럼을 label로 설정\n",
    "'''\n",
    "SEQ_LEN = 800\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 2\n",
    "LR = 1e-5\n",
    "\n",
    "pretrained_path =\"bert\"\n",
    "config_path = os.path.join(pretrained_path, 'bert_config.json')\n",
    "checkpoint_path = os.path.join(pretrained_path, 'bert_model.ckpt')\n",
    "vocab_path = os.path.join(pretrained_path, 'vocab.txt')\n",
    "\n",
    "DATA_COLUMN = \"document\"\n",
    "LABEL_COLUMN = \"label\"\n",
    "\n",
    "'''\n",
    "vocab.txt에 있는 단어에 인덱스를 추가해주는 token_dict 딕셔너리 생성\n",
    "1. 우리가 분석할 문장이 토큰화\n",
    "2. 인덱스로 변경되어 BERT 신경망에 인풋으로 입력\n",
    "'''\n",
    "\n",
    "token_dict = {}\n",
    "with codes.open(vocab_path, 'r', 'utf8') as reader:\n",
    "\tfor line in reader:\n",
    "\t\ttoken = line.strip()\n",
    "\t\tif \"_\" in token:\n",
    "\t\t\ttoken = token.replace(\"_\", \"\")\n",
    "\t\t\ttoken = '##' + token\n",
    "\t\ttoken_dict[token] = len(token_dict)\n",
    "\n",
    "'''\n",
    "1. Tokenizer 상속 후 inherit_Tokennizer 클래스 재정의\n",
    "2. _tokenize 함수를 작성\n",
    "why? -> 영어식으로 쪼개지기때문에 한글화 시키기 위함\n",
    "inherit_Tokennizer는 문장을 토큰화\n",
    "단어의 시작이 아닌 부분엔 ##으로 대체\n",
    "'''\n",
    "class inherit_Tokenizer(Tokenizer):\n",
    "  def _tokenize(self, text):\n",
    "        if not self._cased:\n",
    "            text = text\n",
    "            \n",
    "            text = text.lower()\n",
    "        spaced = ''\n",
    "        for ch in text:\n",
    "            if self._is_punctuation(ch) or self._is_cjk_character(ch):\n",
    "                spaced += ' ' + ch + ' '\n",
    "            elif self._is_space(ch):\n",
    "                spaced += ' '\n",
    "            elif ord(ch) == 0 or ord(ch) == 0xfffd or self._is_control(ch):\n",
    "                continue\n",
    "            else:\n",
    "                spaced += ch\n",
    "        tokens = []\n",
    "        for word in spaced.strip().split():\n",
    "            tokens += self._word_piece_tokenize(word)\n",
    "        return tokens\n",
    "\n",
    "\n",
    "    tokenizer = inherit_Tokenizer(token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ae5edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "데이터를 BERT에 맞는 형식으로 변형해주는 함수 정의\n",
    "tokenizer.encode 함수가 BERT 모형을 토큰화하고, 토큰화된 단어를 인덱스에 맞게 숫자로 변경\n",
    "'''\n",
    "def convert_data(data_df):\n",
    "    global tokenizer\n",
    "    indices, targets = [], []\n",
    "    for i in tqdm(range(len(data_df))):\n",
    "        ids, segments = tokenizer.encode(data_df[DATA_COLUMN][i], max_len=SEQ_LEN)\n",
    "        indices.append(ids)\n",
    "        targets.append(data_df[LABEL_COLUMN][i])\n",
    "    items = list(zip(indices, targets))\n",
    "    \n",
    "    indices, targets = zip(*items)\n",
    "    indices = np.array(indices)\n",
    "    return [indices, np.zeros_like(indices)], np.array(targets)\n",
    "\n",
    "def load_data(pandas_dataframe):\n",
    "    data_df = pandas_dataframe\n",
    "    \n",
    "    data_df[DATA_COLUMN] = data_df[DATA_COLUMN].astype(str)\n",
    "\n",
    "    data_x, data_y = convert_data(data_df)\n",
    "\n",
    "    return data_x, data_y\n",
    "\n",
    "train_x, train_y = load_data(train)\n",
    "test_x, test_y = load_data(test)\n",
    "\n",
    "'''\n",
    "Pre-trained Model의 Input은 토큰화과정에서 숫자로 변형된 것\n",
    "그리고 앞문장인지 뒷문장인지 알려주는지 문장 순서 벡터가 들어감.\n",
    "파인튜닝 시 마스킹을 사용하지 않음\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cb4a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "<BERT input을 문장으로 쓸 경우>\n",
    "윗 설명 예시 코드\n",
    "'''\n",
    "def sentence_convert_data(data):\n",
    "    global tokenizer\n",
    "    indices = []\n",
    "    for i in tqdm(range(len(data))):\n",
    "        print(tokenizer.tokenize(data[i]))\n",
    "        ids, segments = tokenizer.encode(data[i], max_len=SEQ_LEN)\n",
    "        indices.append(ids)\n",
    "        \n",
    "    items = indices\n",
    "    indices = np.array(indices)\n",
    "    return [indices, np.zeros_like(indices)]\n",
    "\n",
    "def sentence_load_data(sentences):#sentence는 List로 받는다\n",
    "           \n",
    "    data_x = sentence_convert_data(sentences)\n",
    "\n",
    "    return data_x\n",
    "\n",
    "sentense_load_data([\"버트 테스트 123 123 123\", \"버트 테스트야 개꿀\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917be610",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-trained model load\n",
    "layer_num = 12\n",
    "model = load_trained_model_from_checkpoint(\n",
    "    config_path,\n",
    "    checkpoint_path,\n",
    "    training=True,\n",
    "    trainable=True,\n",
    "    seq_len=SEQ_LEN,)\n",
    "\n",
    "model.summary() #모델 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d8f8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model tuning\n",
    "'''\n",
    "사전학습 모델 변형\n",
    "input : 토큰 벡터, 세그먼트\n",
    "위와 같은 이유로 inputs = model.inputs[:2]로 정의\n",
    "\n",
    "사전 모델을 변형\n",
    "Dense(1)을 사전 학습 모델에 추가 -> 긍정인지 부정인지 알려줌\n",
    "Dense(1) : 긍정에 가까우면 0, 부정은 1\n",
    "Radam을 사용해 Gradient Descent training\n",
    "output : bert_model\n",
    "'''\n",
    "\n",
    "def get_bert_finetuning_model(model):\n",
    "  inputs = model.inputs[:2]\n",
    "  dense = model.layers[-3].output\n",
    "\n",
    "\n",
    "  outputs = keras.layers.Dense(1, activation='sigmoid',kernel_initializer=keras.initializers.TruncatedNormal(stddev=0.02),\n",
    "                              name = 'real_output')(dense)\n",
    "\n",
    "\n",
    "\n",
    "  bert_model = keras.models.Model(inputs, outputs)\n",
    "  bert_model.compile(\n",
    "      optimizer=RAdam(learning_rate=0.00001, weight_decay=0.0025),\n",
    "      loss='binary_crossentropy',\n",
    "      metrics=['accuracy'])\n",
    "  \n",
    "  return bert_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c53f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model flow check\n",
    "'''\n",
    "<선택>\n",
    "모델 flow 확인\n",
    "'''\n",
    "from IPython.display import SVG\n",
    "from keras.utils import model_to_dot\n",
    "\n",
    "\n",
    "SVG(model_to_dot(get_bert_finetuning_model(model), dpi=65).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f303be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load & fine tuning\n",
    "'''\n",
    "bert_model을 get_bert_finetuning_model 함수로 load\n",
    "bert_model.fit -> 파인튜닝 시작\n",
    "validation_data = (test_x,test_y) -> 실시간으로 테스트 데이터에 대한 정확도를 알기 위함\n",
    "'''\n",
    "sess = K.get_session()\n",
    "uninitialized_variables = set([i.decode('ascii') for i in sess.run(tf.report_uninitialized_variables())])\n",
    "init = tf.variables_initializer([v for v in tf.global_variables() if v.name.split(':')[0] in uninitialized_variables])\n",
    "sess.run(init)\n",
    "\n",
    "bert_model = get_bert_finetuning_model(model)\n",
    "history = bert_model.fit(train_x, train_y, epochs=2, batch_size=16, verbose = 1, validation_data=(test_x, test_y), shuffle=True)\n",
    "\n",
    "#모델 저장\n",
    "bert_model.save_weights(path+\"/bert.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914a3fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bert model load and predict definition\n",
    "'''\n",
    "BERT Model load\n",
    "'''\n",
    "bert_model = get_bert_finetuning_model(model)\n",
    "bert_model.load_weights(path+\"/bert.h5\")\n",
    "\n",
    "'''\n",
    "F1 Score확인\n",
    "Real   Data\n",
    "긍정 -> 진짜 긍정 확률\n",
    "부정 -> 부정 확률\n",
    "\n",
    "predict_convert_data : 테스트 데이터를 input하기 위함. test데이터이기에 label 고려 X\n",
    "'''\n",
    "\n",
    "def predict_convert_data(data_df):\n",
    "    global tokenizer\n",
    "    indices = []\n",
    "    for i in tqdm(range(len(data_df))):\n",
    "        ids, segments = tokenizer.encode(data_df[DATA_COLUMN][i], max_len=SEQ_LEN)\n",
    "        indices.append(ids)\n",
    "        \n",
    "    items = indices\n",
    "    \n",
    "    \n",
    "    indices = np.array(indices)\n",
    "    return [indices, np.zeros_like(indices)]\n",
    "\n",
    "def predict_load_data(x): #Pandas Dataframe을 인풋으로 받는다\n",
    "    data_df = x\n",
    "    data_df[DATA_COLUMN] = data_df[DATA_COLUMN].astype(str)\n",
    "    data_x = predict_convert_data(data_df)\n",
    "\n",
    "    return data_x\n",
    "\n",
    "test_set = predict_load_data(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bfbca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict\n",
    "preds = bert_model.predict(test_set)\n",
    "\n",
    "# 부정 -> 0, 긍정 -> 1 출력\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ead220",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test['label']\n",
    "# F1 Score 확인\n",
    "print(classification_report(y_true, np.round(preds,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef73dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "문장이 입력되면 긍정인지 부정인지 알려주는 함수\n",
    "'''\n",
    "def sentence_convert_data(data):\n",
    "    global tokenizer\n",
    "    indices = []\n",
    "    ids, segments = tokenizer.encode(data, max_len=SEQ_LEN)\n",
    "    indices.append(ids)\n",
    "        \n",
    "    items = indices\n",
    "    indices = np.array(indices)\n",
    "    return [indices, np.zeros_like(indices)]\n",
    "\n",
    "def sentence_evaluation_predict(sentence):\n",
    "    data_x = sentence_convert_data(sentence)\n",
    "    predict = bert_model.predict(data_x)\n",
    "    predict_answer = np.round(np.ravel(predict), 0).item()\n",
    "    \n",
    "    if predict_answer == 0:\n",
    "      print(\"부정적\")\n",
    "    elif predict_answer == 1:\n",
    "      print(\"긍정적\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815ab682",
   "metadata": {},
   "source": [
    "# 데이터 준비\n",
    "+ 기사번호 기사문장 부정적0/긍정적1\n",
    "+ 같은 형식으로 제작해야할듯\n",
    "+ 크롤링 후 수작업 하는 식으로 진행해야할듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd8937c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
